{
    "model": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
    "draft_model": "meta-llama/Llama-3.2-1B-Instruct",
    "offload": false,
    "cuda_graph": true,
    "max_length": 2048,
    "num_cache_layers": 0,
    "generation_length": 256,
    "growmap_path": "../umbrella/trees/sequoia_tree-3x4.json",
    "max_turns": 16,
    "topk": 32,
    "temperature": 0,
    "topp": 0.9,
    "repetition_penalty": 1.0,
    "exit_layer":16,
    "engine": "static",
    "template": "llama3-code"
}
